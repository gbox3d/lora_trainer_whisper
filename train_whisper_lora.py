# train_whisper_lora.py
import os
import argparse
import inspect
from dataclasses import dataclass
from typing import Any, Dict, List

import numpy as np
import torch
from datasets import load_dataset, Audio
from transformers import (
    WhisperProcessor,
    WhisperForConditionalGeneration,
    TrainingArguments,
    Trainer,
    set_seed,
)
from peft import LoraConfig, get_peft_model, TaskType


# -------------------------
# Args
# -------------------------
def parse_args():
    p = argparse.ArgumentParser()

    p.add_argument("--model_name", type=str, default="openai/whisper-small")
    p.add_argument("--manifest", type=str, default="datasets/Sample/manifest.jsonl")
    p.add_argument("--output_dir", type=str, default="outputs/lora")

    p.add_argument("--language", type=str, default="ko")
    p.add_argument("--task", type=str, default="transcribe")

    p.add_argument("--max_steps", type=int, default=300)
    p.add_argument("--batch_size", type=int, default=2)
    p.add_argument("--grad_accum", type=int, default=16)
    p.add_argument("--lr", type=float, default=1e-4)
    p.add_argument("--seed", type=int, default=42)

    # ë©”ëª¨ë¦¬/ì•ˆì •ì„± ì˜µì…˜
    p.add_argument("--fp16", action="store_true")
    p.add_argument("--max_audio_sec", type=float, default=20.0)
    p.add_argument("--use_gradient_checkpointing", action="store_true")
    p.add_argument("--dataloader_workers", type=int, default=0)
    p.add_argument("--pin_memory", action="store_true")

    # LoRA
    p.add_argument("--lora_r", type=int, default=8)
    p.add_argument("--lora_alpha", type=int, default=16)
    p.add_argument("--lora_dropout", type=float, default=0.05)
    p.add_argument("--target_modules", type=str, default="q_proj,v_proj")  # ì½¤ë§ˆ êµ¬ë¶„

    # (ì˜µì…˜) 8bit ë¡œë”© (bitsandbytes í•„ìš”)
    p.add_argument("--load_in_8bit", action="store_true")
    
    p.add_argument("--eval_manifest", type=str, default="", help="ê²€ì¦ìš© manifest.jsonl (ì—†ìœ¼ë©´ trainì—ì„œ ìë™ ë¶„ë¦¬)")
    p.add_argument("--eval_steps", type=int, default=300, help="ëª‡ stepë§ˆë‹¤ eval í• ì§€")
    p.add_argument("--eval_ratio", type=float, default=0.01, help="eval_manifest ì—†ì„ ë•Œ trainì—ì„œ ë¶„ë¦¬í•  ë¹„ìœ¨")


    return p.parse_args()


# -------------------------
# í•µì‹¬ í•´ê²°: Whisper forward í˜¸í™˜ íŒ¨ì¹˜
# -------------------------
def patch_whisper_forward_for_peft(whisper_model: WhisperForConditionalGeneration):
    """
    PEFTê°€ base_model í˜¸ì¶œ ì‹œ input_ids / inputs_embeds ë“±ì„ í‚¤ì›Œë“œë¡œ ë„£ì–´ë„
    Whisperê°€ ì£½ì§€ ì•Šë„ë¡ forwardë¥¼ íŒ¨ì¹˜í•©ë‹ˆë‹¤.

    - input_ids / inputs_embeds: ì œê±° (WhisperëŠ” input_featuresë¥¼ ì‚¬ìš©)
    - signature ê¸°ë°˜ìœ¼ë¡œ Whisper forwardê°€ ì‹¤ì œë¡œ ë°›ëŠ” í‚¤ë§Œ ì „ë‹¬
    - ëª¨ë“ˆ êµ¬ì¡°/ì´ë¦„ì€ ê±´ë“œë¦¬ì§€ ì•ŠìŒ (LoRA ì €ì¥/ë¡œë“œ ì •ìƒ)
    """
    orig_forward = whisper_model.forward  # bound method
    sig = inspect.signature(orig_forward)
    allowed = set(sig.parameters.keys())
    has_varkw = any(p.kind == inspect.Parameter.VAR_KEYWORD for p in sig.parameters.values())

    def patched_forward(*args, **kwargs):
        # PEFTê°€ í•­ìƒ ë„˜ê¸°ëŠ” "Whisperê°€ ì‹«ì–´í•˜ëŠ” í‚¤" ì œê±°
        kwargs.pop("input_ids", None)
        kwargs.pop("inputs_embeds", None)

        # í˜¹ì‹œë¼ë„ ëˆ„êµ°ê°€ input_idsì— ë©œíŠ¹ì§•ì„ ë„£ëŠ” ì´ìƒí•œ ê²½ìš° ëŒ€ë¹„(ì•ˆì „ì¥ì¹˜)
        if "input_features" not in kwargs and "input_ids" in kwargs:
            kwargs["input_features"] = kwargs.pop("input_ids")

        # Whisper forwardê°€ ì‹¤ì œë¡œ ë°›ëŠ” í‚¤ë§Œ ë‚¨ê¸°ê¸°
        if not has_varkw:
            kwargs = {k: v for k, v in kwargs.items() if k in allowed}

        return orig_forward(*args, **kwargs)

    whisper_model.forward = patched_forward


# -------------------------
# Data Collator
# -------------------------
@dataclass
class DataCollatorSpeechSeq2Seq:
    processor: WhisperProcessor
    max_audio_sec: float = 20.0

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        audio_list = []
        for f in features:
            a = f["audio"]["array"]  # datasets.Audio decode ê²°ê³¼
            if isinstance(a, np.ndarray) and a.ndim == 2:  # stereo -> mono
                a = a.mean(axis=1)
            a = np.asarray(a, dtype=np.float32)

            # ê¸¸ì´ ì œí•œ (OOM ë°©ì§€)
            sr = 16000
            max_len = int(self.max_audio_sec * sr)
            if len(a) > max_len:
                a = a[:max_len]

            audio_list.append(a)

        feats = self.processor.feature_extractor(
            audio_list, sampling_rate=16000, return_tensors="pt"
        )

        labels = self.processor.tokenizer(
            [f["text"] for f in features],
            return_tensors="pt",
            padding=True,
        ).input_ids

        labels[labels == self.processor.tokenizer.pad_token_id] = -100

        # Whisperê°€ ë°›ëŠ” í‚¤ë§Œ êµ¬ì„±
        batch = {
            "input_features": feats["input_features"],
            "labels": labels,
        }
        if "attention_mask" in feats:
            batch["attention_mask"] = feats["attention_mask"]

        return batch


# -------------------------
# Trainer (ê¸°ë³¸ Trainerë¡œ ì¶©ë¶„)
# -------------------------
class WhisperTrainer(Trainer):
    # í˜¹ì‹œ Trainer ìª½ì—ì„œ ì´ìƒí•œ í‚¤ë¥¼ ì„ì–´ë„ ë°©ì–´
    def _prepare_inputs(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        inputs = super()._prepare_inputs(inputs)
        # í˜¹ì‹œ ìƒê¸°ë©´ ì œê±° (í•˜ì§€ë§Œ ì´ì œ forward íŒ¨ì¹˜ë¡œë„ ì•ˆì „)
        inputs.pop("input_ids", None)
        inputs.pop("inputs_embeds", None)
        return inputs


# -------------------------
# Main
# -------------------------
def main():
    args = parse_args()
    set_seed(args.seed)
    os.makedirs(args.output_dir, exist_ok=True)

    # Dataset
    # dataset = load_dataset("json", data_files=args.manifest, split="train")
    # dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
    
    # Dataset (Train / Eval ë¶„ë¦¬ ë¡œë“œ)
    train_ds = load_dataset("json", data_files=args.manifest, split="train")
    train_ds = train_ds.cast_column("audio", Audio(sampling_rate=16000))

    if not args.eval_manifest:
        raise ValueError("--eval_manifest ë¥¼ Validation manifest.jsonl ë¡œ ì§€ì •í•´ì£¼ì„¸ìš”.")

    eval_ds = load_dataset("json", data_files=args.eval_manifest, split="train")
    eval_ds = eval_ds.cast_column("audio", Audio(sampling_rate=16000))

    processor = WhisperProcessor.from_pretrained(
        args.model_name,
        language=args.language,
        task=args.task,
    )

    # Model load
    model_kwargs = {}
    if args.load_in_8bit:
        model_kwargs["load_in_8bit"] = True
        model_kwargs["device_map"] = "auto"

    model = WhisperForConditionalGeneration.from_pretrained(args.model_name, **model_kwargs)

    # Whisper í•™ìŠµ ì•ˆì •í™” ì˜µì…˜
    model.config.use_cache = False
    # if args.use_gradient_checkpointing:
    #     model.gradient_checkpointing_enable()
    
    if args.use_gradient_checkpointing:
        model.config.use_cache = False  # ì´ë¯¸ í•˜ì‹œì§€ë§Œ, ì—¬ê¸°ì„œ í™•ì‹¤íˆ
        model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={"use_reentrant": False})

        # (ì•ˆì „ì¥ì¹˜) PEFT + checkpointingì—ì„œ ì…ë ¥ grad ê²½ê³ /ì´ìŠˆ ì˜ˆë°©
        if hasattr(model, "enable_input_require_grads"):
            model.enable_input_require_grads()

    # í•œêµ­ì–´/ì „ì‚¬ ê°•ì œ(ì›í•˜ë©´ ìœ ì§€)
    model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(
        language=args.language, task=args.task
    )

    # âœ… í•µì‹¬: PEFT í˜¸í™˜ forward íŒ¨ì¹˜
    patch_whisper_forward_for_peft(model)

    # LoRA
    target_modules = [s.strip() for s in args.target_modules.split(",") if s.strip()]
    lora_config = LoraConfig(
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        target_modules=target_modules,
        task_type=TaskType.SEQ_2_SEQ_LM,
    )
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    training_args = TrainingArguments(
        output_dir=args.output_dir,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        gradient_accumulation_steps=args.grad_accum,
        learning_rate=args.lr,
        max_steps=args.max_steps,
        fp16=bool(args.fp16),
        ddp_find_unused_parameters=False,

        logging_steps=10,
        save_steps=args.eval_steps,  # âœ… ì…ë ¥ë°›ì€ í‰ê°€ ì£¼ê¸°(500)ì™€ ë™ì¼í•˜ê²Œ ìë™ ì„¤ì •

        # -------------------------------------------------------
        # [ìˆ˜ì •] Tensorboard ì‚¬ìš© ë° ì•ˆì „ì¥ì¹˜
        # -------------------------------------------------------
        report_to=["tensorboard"],                         # âœ… í…ì„œë³´ë“œ í™œì„±í™”
        logging_dir=os.path.join(args.output_dir, "runs"), # âœ… ë¡œê·¸ ê²½ë¡œ

        save_total_limit=5,             # âœ… ìµœëŒ€ 5ê°œ ëª¨ë¸ ë³´ê´€
        load_best_model_at_end=True,    # âœ… í•™ìŠµ ëë‚  ë•Œ ìµœê³  ëª¨ë¸ ìë™ ë¡œë“œ
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        # -------------------------------------------------------

        eval_strategy="steps",
        eval_steps=args.eval_steps,
        
        # report_to="none",  # <--- ğŸ—‘ï¸ ì´ ì¤„ì€ ë°˜ë“œì‹œ ì‚­ì œí•˜ê±°ë‚˜ ì£¼ì„ ì²˜ë¦¬í•˜ì„¸ìš”!

        remove_unused_columns=False,
        dataloader_num_workers=args.dataloader_workers,
        dataloader_pin_memory=bool(args.pin_memory),
    )
    trainer = WhisperTrainer(
        model=model,
        args=training_args,
        train_dataset=train_ds,     # âœ… ë³€ê²½
        eval_dataset=eval_ds,       # âœ… ì¶”ê°€
        data_collator=DataCollatorSpeechSeq2Seq(processor, max_audio_sec=args.max_audio_sec),
    )

    trainer.train()

    # LoRA adapter ì €ì¥
    model.save_pretrained(args.output_dir)
    processor.save_pretrained(args.output_dir)

    print(f"\nâœ… Done. Saved LoRA adapter to: {args.output_dir}\n")


if __name__ == "__main__":
    try:
        main()
    finally:
        import torch.distributed as dist
        if dist.is_available() and dist.is_initialized():
            dist.destroy_process_group()
